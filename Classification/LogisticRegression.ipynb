{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score\n",
    "from multiprocessing import Pool\n",
    "import scipy as sc\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns =100\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from Common import preprocessing,evaluation,CosineClassifier as cos\n",
    "classes_map = {'DOC':0, 'ENTER':1, 'ORG':2, 'PRIV':3, 'RANG':4, 'HOST':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Data//data.txt', delimiter=';', engine='python',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(df.question)\n",
    "questions = preprocessing.preprocess_list(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(df['class'])\n",
    "y = list(map(lambda x: classes_map[x],classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(def_embeddings, y, test_size=0.33, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0,C=10,solver='lbfgs',)).fit(X_train, y_train)\n",
    "ridge = OneVsRestClassifier(RidgeClassifier(random_state=0)).fit(X_train, y_train)\n",
    "svc = OneVsRestClassifier(LinearSVC(random_state=0,)).fit(X_train, y_train)\n",
    "#clf = cos.CosineClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7346750795468233\n",
      "0.7325036348521543\n",
      "0.7404943151159795\n"
     ]
    }
   ],
   "source": [
    "print(evaluation.get_CV_scores(log_reg,X_test,y_test,'f1_weighted').mean())\n",
    "print(evaluation.get_CV_scores(ridge,X_test,y_test,'f1_weighted').mean())\n",
    "print(evaluation.get_CV_scores(svc,X_test,y_test,'f1_weighted').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (question,model):\n",
    "    question = preprocessing.preprocess_list([question])[0]\n",
    "    vect = vectorizer.transform([question])\n",
    "    return model.predict_proba(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 752)\n"
     ]
    }
   ],
   "source": [
    "count_model = CountVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X_ = count_model.fit_transform(questions)\n",
    "Xc = (X_.T * X_)\n",
    "Xc.setdiag(0)\n",
    "print(Xc.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_co_occur = pd.DataFrame(Xc.todense(),columns=count_model.get_feature_names())\n",
    "df_co_occur.index = count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_docs = {}\n",
    "for i in range(Xc.todense().shape[0]):\n",
    "    words = []\n",
    "    for j in range(Xc.todense().shape[1]):\n",
    "        weight = Xc.todense()[i,j]\n",
    "        if weight > 1:\n",
    "            words = words + ([count_model.get_feature_names()[j]]*weight)\n",
    "    pseudo_docs[count_model.get_feature_names()[i]] = words.copy()\n",
    "    words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1582866, 2745800)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec([q.split() for q in questions], size=300, window=10,)\n",
    "model.train([q.split() for q in questions],epochs=200,total_examples=model.corpus_count)\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(\"..//..//web_upos_cbow_300_20_2017.bin.gz\", binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_docs_vec = {}\n",
    "for word in pseudo_docs.keys():\n",
    "    vec = (np.mean([model.wv[w] for w in pseudo_docs[word] if w in model.wv],axis=0))\n",
    "    if not np.isnan(vec).any():\n",
    "        pseudo_docs_vec[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "cnt = 0\n",
    "for q in questions:\n",
    "    vec = [pseudo_docs_vec[w] for w in q.split() if w in list(pseudo_docs_vec.keys())]#*tfidf\n",
    "    if len(vec) < 1:\n",
    "        embeddings.append(np.zeros(300))\n",
    "    else:\n",
    "        embeddings.append(np.array(np.mean(vec,axis=0)))\n",
    "    cnt = cnt +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_embeddings = []\n",
    "cnt = 0\n",
    "for q in questions:\n",
    "    #vec = [model.wv[w]*tfidf[cnt][w] if w in list(tfidf[cnt]) else model.wv[w]*0 for w in q.split() if w in model.wv]\n",
    "    vec = [enth[w][0] for w in q.split() if w in list(enth.keys())]\n",
    "    if len(vec) < 1:\n",
    "        def_embeddings.append(np.zeros(200))\n",
    "    else:\n",
    "        def_embeddings.append(np.array(np.mean(vec,axis=0)))\n",
    "    cnt=cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = []\n",
    "for i in range(X.shape[0]):\n",
    "    dictionary = {}\n",
    "    for j in range(len(vectorizer.get_feature_names())):\n",
    "        dictionary[vectorizer.get_feature_names()[j]] = X[i,j]\n",
    "    tfidf.append(dictionary.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "enthropy = []\n",
    "for q in questions:\n",
    "    question = q.split()\n",
    "    q_dic = Counter(question)\n",
    "    q_len = len(question)\n",
    "    vector = []\n",
    "    cnt=0\n",
    "    for w in vectorizer.get_feature_names():\n",
    "            if w in question:\n",
    "                vector.append(sum(co_occur[cnt])*q_dic[w]/q_len*np.log2(q_dic[w]/q_len))\n",
    "            else:\n",
    "                vector.append(-0.00001*sum(co_occur[cnt]))\n",
    "            cnt=cnt+1\n",
    "    enthropy.append(vector.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=200,algorithm='arpack')\n",
    "matr = svd.fit_transform(np.array(enthropy).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = Normalizer().fit_transform(matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "enth = {}\n",
    "for i in list(zip(vectorizer.get_feature_names(),matr)):\n",
    "    enth[i[0]]=[i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur = Normalizer().fit_transform(Xc.todense()) #MAKE IT PMI(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
