{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score\n",
    "from multiprocessing import Pool\n",
    "import scipy as sc\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns =100\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pickle\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from Common import preprocessing,evaluation,CosineClassifier as cos\n",
    "classes_map = {'DOC':0, 'ENTER':1, 'ORG':2, 'PRIV':3, 'RANG':4, 'HOST':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//Data//data.txt', delimiter=';', engine='python',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(df.question)\n",
    "questions = preprocessing.preprocess_list(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(df['class'])\n",
    "y = list(map(lambda x: classes_map[x],classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = OneVsRestClassifier(LogisticRegression(random_state=0,C=10,solver='lbfgs',)).fit(X_train, y_train)\n",
    "ridge = OneVsRestClassifier(RidgeClassifier(random_state=0)).fit(X_train, y_train)\n",
    "svc = OneVsRestClassifier(LinearSVC(random_state=0,)).fit(X_train, y_train)\n",
    "#clf = cos.CosineClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674157303370787\n",
      "0.9662921348314606\n",
      "0.9685393258426966\n"
     ]
    }
   ],
   "source": [
    "print(evaluation.get_CV_scores(log_reg,X_test,y_test,'precision_micro').mean())\n",
    "print(evaluation.get_CV_scores(ridge,X_test,y_test,'precision_micro').mean())\n",
    "print(evaluation.get_CV_scores(svc,X_test,y_test,'precision_micro').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2076, 2076)\n"
     ]
    }
   ],
   "source": [
    "count_model = CountVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X_ = count_model.fit_transform(questions)\n",
    "Xc = (X_.T * X_)\n",
    "Xc.setdiag(0)\n",
    "print(Xc.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4905495, 10625600)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec([q.split() for q in questions], size=300, window=10,)\n",
    "model.train([q.split() for q in questions],epochs=200,total_examples=model.corpus_count)\n",
    "#model = KeyedVectors.load_word2vec_format(\"..//..//web_upos_cbow_300_20_2017.bin.gz\", binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = []\n",
    "\n",
    "for q in questions:\n",
    "    vec = [model.wv[w] for w in q.split() if w in model.wv]\n",
    "    if len(vec) < 1:\n",
    "        word2vec.append(np.zeros(300))\n",
    "    else:\n",
    "        word2vec.append(np.array(np.mean(vec,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for q in questions:\n",
    "    vec = [enth[w][0] for w in q.split() if w in list(enth.keys())]\n",
    "    if len(vec) < 1:\n",
    "        embeddings.append(np.zeros(300))\n",
    "    else:\n",
    "        embeddings.append(np.array(np.mean(vec,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occur = Normalizer().fit_transform(Xc.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enthropy = []\n",
    "for q in questions:\n",
    "    question = q.split()\n",
    "    q_dic = Counter(question)\n",
    "    q_len = len(question)\n",
    "    vector = []\n",
    "    cnt=0\n",
    "    for w in vectorizer.get_feature_names():\n",
    "            if w in question:\n",
    "                vector.append(sum(co_occur[cnt])*q_dic[w]/q_len*np.log2(q_dic[w]/q_len))\n",
    "            else:\n",
    "                vector.append(-0.00001*sum(co_occur[cnt]))\n",
    "            cnt=cnt+1\n",
    "    enthropy.append(vector.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=400,algorithm='arpack')\n",
    "matr = svd.fit_transform(np.array(enthropy).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = Normalizer().fit_transform(matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enth = {}\n",
    "for i in list(zip(vectorizer.get_feature_names(),matr)):\n",
    "    enth[i[0]]=[i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df).to_csv(\"labels_.tsv\", sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 259: Expected 2 fields in line 259, saw 3\n",
      "Skipping line 288: Expected 2 fields in line 288, saw 3\n",
      "Skipping line 299: Expected 2 fields in line 299, saw 4\n",
      "Skipping line 372: Expected 2 fields in line 372, saw 3\n",
      "Skipping line 374: Expected 2 fields in line 374, saw 3\n",
      "Skipping line 419: Expected 2 fields in line 419, saw 3\n",
      "Skipping line 477: Expected 2 fields in line 477, saw 3\n",
      "Skipping line 527: Expected 2 fields in line 527, saw 3\n",
      "Skipping line 591: Expected 2 fields in line 591, saw 3\n",
      "Skipping line 595: Expected 2 fields in line 595, saw 3\n",
      "Skipping line 696: Expected 2 fields in line 696, saw 3\n",
      "Skipping line 794: Expected 2 fields in line 794, saw 3\n",
      "Skipping line 862: Expected 2 fields in line 862, saw 3\n",
      "Skipping line 878: Expected 2 fields in line 878, saw 3\n",
      "Skipping line 931: Expected 2 fields in line 931, saw 3\n",
      "Skipping line 1004: Expected 2 fields in line 1004, saw 3\n",
      "Skipping line 1114: Expected 2 fields in line 1114, saw 3\n",
      "Skipping line 1307: Expected 2 fields in line 1307, saw 3\n",
      "Skipping line 1407: Expected 2 fields in line 1407, saw 3\n",
      "Skipping line 1412: Expected 2 fields in line 1412, saw 3\n",
      "Skipping line 1701: Expected 2 fields in line 1701, saw 3\n",
      "Skipping line 1785: Expected 2 fields in line 1785, saw 3\n",
      "Skipping line 1799: Expected 2 fields in line 1799, saw 3\n",
      "Skipping line 1911: Expected 2 fields in line 1911, saw 3\n",
      "Skipping line 2022: Expected 2 fields in line 2022, saw 3\n",
      "Skipping line 2040: Expected 2 fields in line 2040, saw 3\n",
      "Skipping line 2097: Expected 2 fields in line 2097, saw 3\n",
      "Skipping line 2312: Expected 2 fields in line 2312, saw 3\n",
      "Skipping line 2347: Expected 2 fields in line 2347, saw 3\n",
      "Skipping line 2423: Expected 2 fields in line 2423, saw 3\n",
      "Skipping line 2470: Expected 2 fields in line 2470, saw 3\n",
      "Skipping line 2500: Expected 2 fields in line 2500, saw 3\n",
      "Skipping line 2528: Expected 2 fields in line 2528, saw 3\n",
      "Skipping line 2563: Expected 2 fields in line 2563, saw 3\n",
      "Skipping line 2564: Expected 2 fields in line 2564, saw 3\n",
      "Skipping line 2565: Expected 2 fields in line 2565, saw 3\n",
      "Skipping line 2708: Expected 2 fields in line 2708, saw 3\n",
      "Skipping line 2779: Expected 2 fields in line 2779, saw 3\n",
      "Skipping line 2798: Expected 2 fields in line 2798, saw 3\n",
      "Skipping line 2858: Expected 2 fields in line 2858, saw 3\n",
      "Skipping line 3154: Expected 2 fields in line 3154, saw 3\n",
      "Skipping line 3175: Expected 2 fields in line 3175, saw 3\n",
      "Skipping line 3677: Expected 2 fields in line 3677, saw 3\n",
      "Skipping line 3782: Expected 2 fields in line 3782, saw 3\n",
      "Skipping line 3814: Expected 2 fields in line 3814, saw 3\n",
      "Skipping line 3838: Expected 2 fields in line 3838, saw 3\n",
      "Skipping line 3853: Expected 2 fields in line 3853, saw 3\n",
      "Skipping line 3859: Expected 2 fields in line 3859, saw 3\n",
      "Skipping line 3869: Expected 2 fields in line 3869, saw 3\n",
      "Skipping line 4045: Expected 2 fields in line 4045, saw 3\n",
      "Skipping line 4147: Expected 2 fields in line 4147, saw 3\n",
      "Skipping line 4202: Expected 2 fields in line 4202, saw 3\n",
      "Skipping line 4517: Expected 2 fields in line 4517, saw 3\n",
      "Skipping line 4819: Expected 2 fields in line 4819, saw 3\n",
      "Skipping line 4939: Expected 2 fields in line 4939, saw 3\n",
      "Skipping line 5074: Expected 2 fields in line 5074, saw 3\n",
      "Skipping line 5214: Expected 2 fields in line 5214, saw 3\n",
      "Skipping line 5308: Expected 2 fields in line 5308, saw 3\n",
      "Skipping line 5357: Expected 2 fields in line 5357, saw 3\n",
      "Skipping line 5382: Expected 2 fields in line 5382, saw 3\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..//Data//ccg.txt',sep=':', delimiter=':', engine='python',encoding='utf8',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = np.array(df.question)\n",
    "questions = preprocessing.preprocess_eng_list(questions)\n",
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(questions)\n",
    "classes = np.array(df['class'])\n",
    "y = list(map(lambda x: classes_map2[x],classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_map2 = {'ENTY':0,'HUM':1,'DESC':2,'NUM':3,'LOC':4,'ABBR':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5392, 2076)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
